{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## lyrics\n",
        "start                  recovery dally jove shouldst"
      ],
      "metadata": {
        "id": "bkvInlVWB0dZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NLUzSd3cC-HB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e1e736c-422c-4fee-f131-5864fd93e2fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 드라이브 마운트\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob #glob 모듈의 glob 함수는 사용자가 제시한 조건에 맞는 파일명을 리스트 형식으로 반환한다\n",
        "import os\n",
        "\n",
        "lyrics_file_path = '/content/drive/MyDrive/Colab Notebooks/Data/shakespeare.txt'\n",
        "\n",
        "txt_list = glob.glob(lyrics_file_path) #txt_file_path 경로에 있는 모든 파일명을 리스트 형식으로 txt_list 에 할당"
      ],
      "metadata": {
        "id": "YyupdjDIHLID"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_corpus = []\n",
        "\n",
        "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines() #read() : 파일 전체의 내용을 하나의 문자열로 읽어온다. , splitlines()  : 여러라인으로 구분되어 있는 문자열을 한라인씩 분리하여 리스트로 반환\n",
        "        raw_corpus.extend(raw) # extend() : 리스트함수로 추가적인 내용을 연장 한다.\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:3])\n"
      ],
      "metadata": {
        "id": "ehHV3qjKHMLj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8071ca03-52ec-4f3d-f006-018444ac5916"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 크기: 40000\n",
            "Examples:\n",
            " ['First Citizen:', 'Before we proceed any further, hear me speak.', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip() # 1\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
        "    sentence = sentence.strip() # 5\n",
        "    sentence = ' ' + sentence + ' ' # 6\n",
        "    return sentence\n",
        "\n",
        "corpus = list(map(preprocess_sentence, raw_corpus))"
      ],
      "metadata": {
        "id": "gfLlML-YHNHw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# 토크나이저 함수로 Tensor 변환\n",
        "\n",
        "def tokenize(corpus):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=40000,\n",
        "        filters=' ',\n",
        "        oov_token=\"\"\n",
        "    )\n",
        "\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post',maxlen=15)\n",
        "\n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)"
      ],
      "metadata": {
        "id": "8ARFjd0WHOLh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0039d680-ec1d-4699-8eb6-f0aae69705e6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  98  281    0 ...    0    0    0]\n",
            " [ 150   38  982 ...    0    0    0]\n",
            " [   0    0    0 ...    0    0    0]\n",
            " ...\n",
            " [ 166  582    2 ...    0    0    0]\n",
            " [  31   69  145 ...    0    0    0]\n",
            " [1074   31  141 ...    0    0    0]] <keras.src.legacy.preprocessing.text.Tokenizer object at 0x7cf085a9c740>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "enc_inputs = tensor[:, :-1]\n",
        "dec_targets = tensor[:, 1:]\n",
        "\n",
        "# 20%를 평가 데이터로 분리\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(\n",
        "    enc_inputs,\n",
        "    dec_targets,\n",
        "    test_size=0.2,\n",
        "    random_state=42,  # 재현성 확보 위해 시드 고정 (선택 사항)\n",
        "    shuffle=True      # 데이터 섞기\n",
        ")"
      ],
      "metadata": {
        "id": "pnYwDrnjHP7m"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_train"
      ],
      "metadata": {
        "id": "reYD5K7pHQ6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d999422d-cbb3-4402-aa41-0c51b5b35f34"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[480,   6,  96, ...,   0,   0,   0],\n",
              "       [ 66,  61,  23, ...,   0,   0,   0],\n",
              "       [101,   5,  33, ...,   0,   0,   0],\n",
              "       ...,\n",
              "       [  0,   0,   0, ...,   0,   0,   0],\n",
              "       [ 98, 691,   0, ...,   0,   0,   0],\n",
              "       [  0,   0,   0, ...,   0,   0,   0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(enc_train)\n",
        "BATCH_SIZE = 32\n",
        "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
        "\n",
        "VOCAB_SIZE = tokenizer.num_words + 1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n",
        "val_dataset = val_dataset.shuffle(BUFFER_SIZE)\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(val_dataset)"
      ],
      "metadata": {
        "id": "fteiP08sHSah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "816fc996-d7b8-4192-cb60-c32832cf9f71"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_BatchDataset element_spec=(TensorSpec(shape=(32, 14), dtype=tf.int32, name=None), TensorSpec(shape=(32, 14), dtype=tf.int32, name=None))>\n",
            "<_BatchDataset element_spec=(TensorSpec(shape=(32, 14), dtype=tf.int32, name=None), TensorSpec(shape=(32, 14), dtype=tf.int32, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        # Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성되어 있다.\n",
        "        # Embedding 레이어는 단어 사전의 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔준다.\n",
        "        # 이 워드 벡터는 의미 벡터 공간에서 단어의 추상적 표현으로 사용된다.\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out\n",
        "# embedding size 값이 커질수록 단어의 추상적인 특징들을 더 잡아낼 수 있지만\n",
        "# 그만큼 충분한 데이터가 없으면 안좋은 결과 값을 가져옵니다!\n",
        "embedding_size = 256 # 워드 벡터의 차원수를 말하며 단어가 추상적으로 표현되는 크기입니다.\n",
        "hidden_size = 1024 # 모델에 얼마나 많은 일꾼을 둘 것인가? 정도로 이해하면 좋다.\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size) # tokenizer.num_words에 +1인 이유는 문장에 없는 pad가 사용되었기 때문이다."
      ],
      "metadata": {
        "id": "yIoqgWUOHUZN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "optimizer = tf.keras.optimizers.Adam() # Adam은 현재 가장 많이 사용하는 옵티마이저이다. 자세한 내용은 차차 배운다.\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy( # 훈련 데이터의 라벨이 정수의 형태로 제공될 때 사용하는 손실함수이다.\n",
        "    from_logits=True, # 기본값은 False이다. 모델에 의해 생성된 출력 값이 정규화되지 않았음을 손실 함수에 알려준다. 즉 softmax함수가 적용되지 않았다는걸 의미한다.\n",
        "    reduction='none'  # 기본값은 SUM이다. 각자 나오는 값의 반환 원할 때 None을 사용한다.\n",
        ")\n",
        "# 모델을 학습시키키 위한 학습과정을 설정하는 단계이다.\n",
        "model.compile(loss=loss, optimizer=optimizer) # 손실함수와 훈련과정을 설정했다.\n",
        "history = model.fit(\n",
        "    dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=5\n",
        ")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msEUtteHHrH1",
        "outputId": "097a29f3-8403-487e-9dd9-8e9bbaaf8d9d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 237ms/step - loss: 3.3206 - val_loss: 2.3174\n",
            "Epoch 2/5\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 243ms/step - loss: 2.1900 - val_loss: 2.1994\n",
            "Epoch 3/5\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 244ms/step - loss: 2.0109 - val_loss: 2.1606\n",
            "Epoch 4/5\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 244ms/step - loss: 1.8597 - val_loss: 2.1542\n",
            "Epoch 5/5\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 244ms/step - loss: 1.7075 - val_loss: 2.1736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_YTLe4YBVEa"
      },
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"\", max_len=30, temperature=1.0):\n",
        "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "\n",
        "    # Check if '' is in word_index, if not, handle it (e.g., set a default or skip)\n",
        "    end_token = tokenizer.word_index.get(\"\", None)\n",
        "\n",
        "    while True:\n",
        "        # 1. 모델 예측 (logits)\n",
        "        predict = model(test_tensor)\n",
        "        # 2. Temperature를 적용하여 예측된 확률 분포 조절\n",
        "        predict = predict / temperature\n",
        "        # 3. Softmax를 적용하여 확률 분포 얻기\n",
        "        predict_word_probabilities = tf.nn.softmax(predict, axis=-1)\n",
        "        # 4. 확률 분포에서 다음 단어 샘플링\n",
        "        predict_word = tf.random.categorical(predict_word_probabilities[:, -1], num_samples=1)[0].numpy()\n",
        "        predict_word = predict_word[0] # tf.random.categorical returns a 2D tensor, get the scalar\n",
        "\n",
        "        # 5. 샘플링된 단어 인덱스를 텐서로 확장\n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(tf.constant([predict_word], dtype=tf.int64), axis=0)], axis=-1)\n",
        "\n",
        "        # 6. 종료 조건 확인\n",
        "        if end_token is not None and predict_word == end_token:\n",
        "            break\n",
        "        if test_tensor.shape[1] >= max_len:\n",
        "            break\n",
        "\n",
        "    generated = \"\"\n",
        "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다\n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        if word_index in tokenizer.index_word:\n",
        "            generated += tokenizer.index_word[word_index] + \" \"\n",
        "        else:\n",
        "            generated += \" \"\n",
        "\n",
        "    return generated.strip()\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, init_sentence=\" start \", max_len=30)"
      ],
      "metadata": {
        "id": "z_vQqinjHYFk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1dadac2f-a598-4b18-cf07-47cb5b8378b3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'start                  recovery dally jove shouldst'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dQfQ_qfiH1ac"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}