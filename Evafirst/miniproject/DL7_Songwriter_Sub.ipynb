{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## lyrics\n",
        "\n",
        "start holiday embrace mariana climate besiege tied commendable slight manners logic bastards parents toads pilate ten gains denying george withdraw spices muse abbot painting galled heel beggarly proves canon acknowledge harvest tapster avoided aqua gyves dozen gallops wealthy provide offences hair pence thoroughly bustle lean forbidden parlous mock seek smells thinking petition shun method madly female wrinkle blessed rare fondly"
      ],
      "metadata": {
        "id": "bkvInlVWB0dZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "NLUzSd3cC-HB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a7ab78e-6815-4380-8c0a-8d20a34f95b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 드라이브 마운트\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob #glob 모듈의 glob 함수는 사용자가 제시한 조건에 맞는 파일명을 리스트 형식으로 반환한다\n",
        "import os\n",
        "\n",
        "lyrics_file_path = '/content/drive/MyDrive/Colab Notebooks/Data/shakespeare.txt'\n",
        "\n",
        "txt_list = glob.glob(lyrics_file_path) #txt_file_path 경로에 있는 모든 파일명을 리스트 형식으로 txt_list 에 할당"
      ],
      "metadata": {
        "id": "YyupdjDIHLID"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_corpus = []\n",
        "\n",
        "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines() #read() : 파일 전체의 내용을 하나의 문자열로 읽어온다. , splitlines()  : 여러라인으로 구분되어 있는 문자열을 한라인씩 분리하여 리스트로 반환\n",
        "        raw_corpus.extend(raw) # extend() : 리스트함수로 추가적인 내용을 연장 한다.\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:3])\n"
      ],
      "metadata": {
        "id": "ehHV3qjKHMLj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de1646a5-022c-45c3-e17f-0ab4c123d2b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 크기: 40000\n",
            "Examples:\n",
            " ['First Citizen:', 'Before we proceed any further, hear me speak.', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip() # 1\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
        "    sentence = sentence.strip() # 5\n",
        "    sentence = ' ' + sentence + ' ' # 6\n",
        "    return sentence\n",
        "\n",
        "corpus = list(map(preprocess_sentence, raw_corpus))"
      ],
      "metadata": {
        "id": "gfLlML-YHNHw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# 토크나이저 함수로 Tensor 변환\n",
        "\n",
        "def tokenize(corpus):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=20000,\n",
        "        filters=' ',\n",
        "        oov_token=\"\"\n",
        "    )\n",
        "\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post',maxlen=15)\n",
        "\n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)"
      ],
      "metadata": {
        "id": "8ARFjd0WHOLh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed09ddf7-e33d-4576-e865-3f9286f09049"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  98  281    0 ...    0    0    0]\n",
            " [ 150   38  982 ...    0    0    0]\n",
            " [   0    0    0 ...    0    0    0]\n",
            " ...\n",
            " [ 166  582    2 ...    0    0    0]\n",
            " [  31   69  145 ...    0    0    0]\n",
            " [1074   31  141 ...    0    0    0]] <keras.src.legacy.preprocessing.text.Tokenizer object at 0x7ba4e1044740>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "enc_inputs = tensor[:, :-1]\n",
        "dec_targets = tensor[:, 1:]\n",
        "\n",
        "# 20%를 평가 데이터로 분리\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(\n",
        "    enc_inputs,\n",
        "    dec_targets,\n",
        "    test_size=0.2,\n",
        "    random_state=42,  # 재현성 확보 위해 시드 고정 (선택 사항)\n",
        "    shuffle=True      # 데이터 섞기\n",
        ")"
      ],
      "metadata": {
        "id": "pnYwDrnjHP7m"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_train"
      ],
      "metadata": {
        "id": "reYD5K7pHQ6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d92057b0-c4e9-4480-8d3d-7051c797a099"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[480,   6,  96, ...,   0,   0,   0],\n",
              "       [ 66,  61,  23, ...,   0,   0,   0],\n",
              "       [101,   5,  33, ...,   0,   0,   0],\n",
              "       ...,\n",
              "       [  0,   0,   0, ...,   0,   0,   0],\n",
              "       [ 98, 691,   0, ...,   0,   0,   0],\n",
              "       [  0,   0,   0, ...,   0,   0,   0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(enc_train)\n",
        "BATCH_SIZE = 128\n",
        "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
        "\n",
        "VOCAB_SIZE = tokenizer.num_words + 1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n",
        "val_dataset = val_dataset.shuffle(BUFFER_SIZE)\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(val_dataset)"
      ],
      "metadata": {
        "id": "fteiP08sHSah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "896d2274-e6ec-43cd-87f6-330f386fe55f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_BatchDataset element_spec=(TensorSpec(shape=(128, 14), dtype=tf.int32, name=None), TensorSpec(shape=(128, 14), dtype=tf.int32, name=None))>\n",
            "<_BatchDataset element_spec=(TensorSpec(shape=(128, 14), dtype=tf.int32, name=None), TensorSpec(shape=(128, 14), dtype=tf.int32, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        # Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성되어 있다.\n",
        "        # Embedding 레이어는 단어 사전의 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔준다.\n",
        "        # 이 워드 벡터는 의미 벡터 공간에서 단어의 추상적 표현으로 사용된다.\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out\n",
        "# embedding size 값이 커질수록 단어의 추상적인 특징들을 더 잡아낼 수 있지만\n",
        "# 그만큼 충분한 데이터가 없으면 안좋은 결과 값을 가져옵니다!\n",
        "embedding_size = 256 # 워드 벡터의 차원수를 말하며 단어가 추상적으로 표현되는 크기입니다.\n",
        "hidden_size = 1024 # 모델에 얼마나 많은 일꾼을 둘 것인가? 정도로 이해하면 좋다.\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size) # tokenizer.num_words에 +1인 이유는 문장에 없는 pad가 사용되었기 때문이다."
      ],
      "metadata": {
        "id": "yIoqgWUOHUZN"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_YTLe4YBVEa",
        "outputId": "3789c23a-0e37-4a22-d65e-11d48be6d93e"
      },
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"\", max_len=30, temperature=1.0):\n",
        "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "\n",
        "    # Check if '' is in word_index, if not, handle it (e.g., set a default or skip)\n",
        "    end_token = tokenizer.word_index.get(\"\", None)\n",
        "\n",
        "    while True:\n",
        "        # 1. 모델 예측 (logits)\n",
        "        predict = model(test_tensor)\n",
        "        # 2. Temperature를 적용하여 예측된 확률 분포 조절\n",
        "        predict = predict / temperature\n",
        "        # 3. Softmax를 적용하여 확률 분포 얻기\n",
        "        predict_word_probabilities = tf.nn.softmax(predict, axis=-1)\n",
        "        # 4. 확률 분포에서 다음 단어 샘플링\n",
        "        predict_word = tf.random.categorical(predict_word_probabilities[:, -1], num_samples=1)[0].numpy()\n",
        "        predict_word = predict_word[0] # tf.random.categorical returns a 2D tensor, get the scalar\n",
        "\n",
        "        # 5. 샘플링된 단어 인덱스를 텐서로 확장\n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(tf.constant([predict_word], dtype=tf.int64), axis=0)], axis=-1)\n",
        "\n",
        "        # 6. 종료 조건 확인\n",
        "        if end_token is not None and predict_word == end_token:\n",
        "            break\n",
        "        if test_tensor.shape[1] >= max_len:\n",
        "            break\n",
        "\n",
        "    generated = \"\"\n",
        "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다\n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        if word_index in tokenizer.index_word:\n",
        "            generated += tokenizer.index_word[word_index] + \" \"\n",
        "        else:\n",
        "            generated += \" \"\n",
        "\n",
        "    return generated.strip()\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generate_text function updated to include temperature sampling and fixed concat rank mismatch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, init_sentence=\" start \", max_len=60)"
      ],
      "metadata": {
        "id": "z_vQqinjHYFk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "07264513-3a6d-4367-f3a7-5b9cdefc0c5b"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'start holiday embrace mariana climate besiege tied commendable slight manners logic bastards parents toads pilate ten gains denying george withdraw spices muse abbot painting galled heel beggarly proves canon acknowledge harvest tapster avoided aqua gyves dozen gallops wealthy provide offences hair pence thoroughly bustle lean forbidden parlous mock seek smells thinking petition shun method madly female wrinkle blessed rare fondly'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    }
  ]
}